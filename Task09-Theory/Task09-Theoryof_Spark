What are RDD Operations in Spark? 
There are two type of RDD operations, they are
Transformation
Narrow Transformation
map()
filter()
Wide Transformation
groupbyKey()
reducebyKey()
Actions
Lazy Operation
Spark drivers and external storage systems store the value of action. It brings laziness of RDD into motion.

2. What do you understand by Lazy Evaluation? 
As the name itself indicates its definition, lazy evaluation in Spark means that the execution will not start until an action is triggered. In Spark, the picture of lazy evaluation comes when Spark transformations occur.


2.What is a DAG in spark ? 
DAG (Directed Acyclic Graph) in Apache Spark is an alternative to MapReduce. It is a programming style used in distributed systems. In MapReduce, we just have two functions (map and reduce), while DAG has multiple levels that form a tree structure. Hence, DAG execution is faster than MapReduce because intermediate results do not write to disk.

What is the role of a spark Driver ? 
The spark driver is the program that defines the transformations and actions on RDDs of knowledge and submits requests to the master. Spark driver is a program that runs on the master node of the machine which declares transformations and actions on knowledge RDDs.
In easy terms, the driver in Spark creates SparkContext, connected to a given Spark Master.It conjointly delivers the RDD graphs to Master, wherever the standalone cluster manager runs

What is Shuffling in Spark ? 
Shuffle operation is used in Spark to re-distribute data across multiple partitions. It is a costly and complex operation.
In general a single task in Spark operates on elements in one partition. To execute shuffle, we have to run an operation on all elements of all partitions. It is also called all-to- all operation.

What are the deploy modes in Spark? 
Two deployment modes can be used to launch Spark applications on YARN:
In cluster mode:
Jobs are managed by the YARN cluster. The Spark driver runs inside an Application Master (AM) process that is managed by YARN. This means that the client can go away after initiating the application.
In client mode: 
The Spark driver runs in the client process, and the Application Master is used only to request resources from YARN.

What is the difference between RDD and a dataframe ?
DataFrame: A Data Frame is used for storing data in tables. It is equivalent to a table in a relational database but with richer optimization. It is a data abstraction and domain-specific language (DSL) applicable to a structure and semi-structured data. It is a distributed collection of data in the form of named column and row. It has a matrix-like structure whose column may be different types (numeric, logical, factor, or character ).we can say a data frame has a two-dimensional array like structure where each column contains the value of one variable and row contains one set of values for each column. It combines features of list and matrices.

RDD: is the representation of a set of records, immutable collection of objects with distributed computing. RDD is a large collection of data or RDD is an array of reference for partitioned objects. Each and every dataset in RDD is logically partitioned across many servers so that they can be computed on different nodes of the cluster. RDDs are fault tolerant i.e. self-recovered/recomputed in the case of failure. The dataset could be data loaded externally by the users which can be in the form of JSON file, CSV file, text file or database via JDBC with no specific data structure.


How does spark achieve fault tolerance ? 
If any of the nodes of processing data gets crashed, that results in a fault in a cluster. In other words, RDD is logically partitioned and each node is operating on a partition at any point in time. Operations which are being performed is a series of scala functions. Those operations are being executed on that partition of RDD. This series of operations are merged together and create a DAG, it refers to Directed Acyclic Graph. That means DAG keeps track of operations performed.

If any node crashes in the middle of an operation, the cluster manager finds out that node. Then, it tries to assign another node to continue the processing at the same place. This node will operate on the same partition of RDD and series of operations. Due to this new node, there is effectively no data loss. Meanwhile, that new node continues the processing smoothly.
